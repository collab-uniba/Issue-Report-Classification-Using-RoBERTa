{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b430e366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ekphrasis\n",
    "!python -m pip install transformers\n",
    "!python -m pip install imblearn\n",
    "!python -m pip install pandas\n",
    "!python -m pip install sklearn\n",
    "!python -m pip install numpy\n",
    "!python -m pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!python -m pip install textattack\n",
    "!python -m pip install transformers\n",
    "!python -m pip install ipywidgets==7.4.2\n",
    "!python -m pip install mlflow\n",
    "!python -m pip install optuna datasets scipy\n",
    "!python -m pip install evaluate\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import re\n",
    "from _datetime import datetime as dt\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import json\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b006440",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "img_1 = re.compile('!\\[(.*)\\]\\(.*\\)')\n",
    "link_1 = re.compile('\\[(.*)\\]\\(.*\\)')\n",
    "link_2 = re.compile('\\[(.*)\\]: [^\\s]+')\n",
    "code_1 = re.compile('(:?`[^`]+`|```[^`]*```)')\n",
    "\n",
    "def preprocess(row):\n",
    "  doc = \"\"\n",
    "  doc += str(row.issue_title)\n",
    "  doc += \" \"\n",
    "  doc += str(row.issue_body)\n",
    "  \n",
    "  return clean_text(doc)\n",
    "\n",
    "def clean_text(text):\n",
    "  cleaned = re.sub(img_1, r'\\1 <img>', text)\n",
    "  cleaned = re.sub(link_1, r'\\1 <url>', cleaned)\n",
    "  cleaned = re.sub(link_2, r'\\1 <url>', cleaned)\n",
    "  cleaned = re.sub(code_1, '<code>', cleaned)\n",
    "  ekph_cleaned = \" \".join(text_processor.pre_process_doc(cleaned))\n",
    "  return ekph_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393056b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rows(df, label_encoder):\n",
    "    df = df.fillna({\n",
    "                        'issue_title': '',\n",
    "                        'issue_body':''                  \n",
    "                   })\n",
    "    df['text'] = df['issue_title'] + df['issue_body']\n",
    "    df['label'] = label_encoder.transform(df['issue_label'])\n",
    "    df = df.filter(['text', 'label'])\n",
    "    df['text'] = [clean_text(text) for text in tqdm(df['text'])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466fdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_to_dataframe(str_representation_of_report):\n",
    "    split_string = [x.split(' ') for x in str_representation_of_report.split('\\n')]\n",
    "    column_names = ['']+[x for x in split_string[0] if x!='']\n",
    "    values = []\n",
    "    for table_row in split_string[1:-1]:\n",
    "        table_row = [value for value in table_row if value!='']\n",
    "        if table_row!=[]:\n",
    "            values.append(table_row)\n",
    "    for i in values:\n",
    "        for j in range(len(i)):\n",
    "            if i[1] == 'avg':\n",
    "                i[0:2] = [' '.join(i[0:2])]\n",
    "            if len(i) == 3:\n",
    "                i.insert(1,np.nan)\n",
    "                i.insert(2, np.nan)\n",
    "            else:\n",
    "                pass\n",
    "    report_to_df = pd.DataFrame(data=values, columns=column_names)\n",
    "    return report_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d9d3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the training set if it does not exist\n",
    "if not os.path.isfile(\"github-labels-top3-803k-train.csv\"):\n",
    "  !curl \"https://tickettagger.blob.core.windows.net/datasets/github-labels-top3-803k-train.tar.gz\" | tar -xz\n",
    "\n",
    "trainset = pd.read_csv(\"github-labels-top3-803k-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06bf8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenc = LabelEncoder()\n",
    "lenc.fit(trainset[\"issue_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8eb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.drop_duplicates(subset=['issue_url'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2facca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = trainset.groupby(\"issue_label\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce0fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the dataset into training and evaluation sets while preserving label distribution\n",
    "train_dataset, eval_dataset = train_test_split(\n",
    "    trainset, test_size=0.1, stratify=trainset['issue_label'], random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = preprocess_rows(train_dataset, lenc)\n",
    "eval_set = preprocess_rows(eval_dataset, lenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "print('Using seed: {}'.format(seed_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(trainset['issue_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93559baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_set = Dataset.from_pandas(train_set)\n",
    "eval_set = Dataset.from_pandas(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d60c688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'roberta-base'\n",
    "NUM_LABELS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b68c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, model_max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bb47a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_eval = eval_set.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbaee021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WEIGHT_DECAY= 1e-8\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "059ab589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "f1_metric = evaluate.load(\"f1\", average='micro')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return f1_metric.compute(predictions=predictions, references=labels, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "516c5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=NUM_LABELS, \n",
    "                                                           max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ddd4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                weight_decay=WEIGHT_DECAY,\n",
    "                                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                                  num_train_epochs=4,\n",
    "                                seed = seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347de2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    model_init=model_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7150ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 4e-5, 3e-5, 2e-5]),\n",
    "        #\"per_device_train_batch_size\": trial.suggest_categorical(\"num_train_epochs\", [1,2,3,4]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7924e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default objective is the sum of all metrics\n",
    "# when metrics are provided, so we have to maximize it.\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    hp_space=optuna_hp_space,\n",
    "    direction=\"maximize\", \n",
    "    backend=\"optuna\", \n",
    "    n_trials=5, # number of trials\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
