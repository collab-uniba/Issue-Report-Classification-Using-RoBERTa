{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b430e366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870a6004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 26 15:42:08 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:25:00.0 Off |                    0 |\r\n",
      "| N/A   24C    P0    31W / 250W |      0MiB / 40536MiB |      0%      Default |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ekphrasis\n",
    "!python -m pip install transformers\n",
    "!python -m pip install imblearn\n",
    "!python -m pip install pandas\n",
    "!python -m pip install sklearn\n",
    "!python -m pip install numpy\n",
    "!python -m pip install keras\n",
    "!python -m pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!python -m pip install textattack\n",
    "!python -m pip install transformers\n",
    "!python -m pip install ipywidgets==7.4.2\n",
    "!python -m pip install mlflow\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4f9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import re\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from _datetime import datetime as dt\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import json\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b006440",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "img_1 = re.compile('!\\[(.*)\\]\\(.*\\)')\n",
    "link_1 = re.compile('\\[(.*)\\]\\(.*\\)')\n",
    "link_2 = re.compile('\\[(.*)\\]: [^\\s]+')\n",
    "code_1 = re.compile('(:?`[^`]+`|```[^`]*```)')\n",
    "\n",
    "def preprocess(row):\n",
    "  doc = \"\"\n",
    "  doc += str(row.issue_title)\n",
    "  doc += \" \"\n",
    "  doc += str(row.issue_body)\n",
    "  \n",
    "  return clean_text(doc)\n",
    "\n",
    "def clean_text(text):\n",
    "  cleaned = re.sub(img_1, r'\\1 <img>', text)\n",
    "  cleaned = re.sub(link_1, r'\\1 <url>', cleaned)\n",
    "  cleaned = re.sub(link_2, r'\\1 <url>', cleaned)\n",
    "  cleaned = re.sub(code_1, '<code>', cleaned)\n",
    "  ekph_cleaned = \" \".join(text_processor.pre_process_doc(cleaned))\n",
    "  return ekph_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393056b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rows(df, label_encoder):\n",
    "    df = df.fillna({\n",
    "                        'issue_title': '',\n",
    "                        'issue_body':''                  \n",
    "                   })\n",
    "    df['text'] = df['issue_title'] + ' ' + df['issue_body']\n",
    "    df['label'] = label_encoder.transform(df['issue_label'])\n",
    "    df = df.filter(['text', 'label'])\n",
    "    df['text'] = [clean_text(text) for text in tqdm(df['text'])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466fdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_to_dataframe(str_representation_of_report):\n",
    "    split_string = [x.split(' ') for x in str_representation_of_report.split('\\n')]\n",
    "    column_names = ['']+[x for x in split_string[0] if x!='']\n",
    "    values = []\n",
    "    for table_row in split_string[1:-1]:\n",
    "        table_row = [value for value in table_row if value!='']\n",
    "        if table_row!=[]:\n",
    "            values.append(table_row)\n",
    "    for i in values:\n",
    "        for j in range(len(i)):\n",
    "            if i[1] == 'avg':\n",
    "                i[0:2] = [' '.join(i[0:2])]\n",
    "            if len(i) == 3:\n",
    "                i.insert(1,np.nan)\n",
    "                i.insert(2, np.nan)\n",
    "            else:\n",
    "                pass\n",
    "    report_to_df = pd.DataFrame(data=values, columns=column_names)\n",
    "    return report_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9d3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the training set if it does not exist\n",
    "if not os.path.isfile(\"github-labels-top3-803k-train.csv\"):\n",
    "  !curl \"https://tickettagger.blob.core.windows.net/datasets/github-labels-top3-803k-train.tar.gz\" | tar -xz\n",
    "\n",
    "trainset = pd.read_csv(\"github-labels-top3-803k-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4af67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"github-labels-top3-803k-test.csv\"):\n",
    "  !curl \"https://tickettagger.blob.core.windows.net/datasets/github-labels-top3-803k-test.tar.gz\" | tar -xz\n",
    "\n",
    "testset = pd.read_csv(\"github-labels-top3-803k-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06bf8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenc = LabelEncoder()\n",
    "lenc.fit(trainset[\"issue_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b8eb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.drop_duplicates(subset=['issue_url'], inplace=True)\n",
    "testset.drop_duplicates(subset=['issue_url'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba7d0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\dict.json') as f:\n",
    "    memo_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "print('Using seed: {}'.format(seed_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903da128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "STAR_FILTER = 1500\n",
    "\n",
    "stars = []\n",
    "\n",
    "temp = trainset.copy()\n",
    "for ix, x in tqdm(temp.iterrows()):\n",
    "    stars.append(memo_dict[x['repository_url']][0])\n",
    "\n",
    "temp['stars'] = stars\n",
    "\n",
    "filtered_df_train = temp.loc[(temp['stars']>= STAR_FILTER)]\n",
    "\n",
    "starss = []\n",
    "\n",
    "tempp = testset.copy()\n",
    "for ix, x in tqdm(tempp.iterrows()):\n",
    "    starss.append(memo_dict[x['repository_url']][0])\n",
    "\n",
    "tempp['stars'] = starss\n",
    "\n",
    "filtered_df_test = tempp.loc[(tempp['stars']>= STAR_FILTER)]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(trainset['issue_label'])\n",
    "\n",
    "f_train_size = filtered_df_train.groupby(\"issue_label\").size()\n",
    "f_test_size = filtered_df_test.groupby(\"issue_label\").size()\n",
    "\n",
    "f_under_train_dict = {\n",
    "    \"bug\" : f_train_size['question'],\n",
    "    \n",
    "    \"enhancement\" : f_train_size['question'],\n",
    "    \n",
    "    \"question\" : f_train_size['question'],\n",
    "}\n",
    "\n",
    "f_test_set = preprocess_rows(filtered_df_test, label_encoder)\n",
    "\n",
    "n_train_set, _ = RandomUnderSampler(sampling_strategy=f_under_train_dict, random_state=42).fit_resample(trainset, list(trainset[\"issue_label\"]))\n",
    "\n",
    "under_test_dict = {\n",
    "    \n",
    "    \"bug\" : f_test_size['bug'],\n",
    "    \n",
    "    \"enhancement\" : f_test_size['enhancement'],\n",
    "    \n",
    "    \"question\" : f_test_size['question'],\n",
    "}\n",
    "\n",
    "n_train_set = preprocess_rows(n_train_set, label_encoder)\n",
    "\n",
    "f_test_set = Dataset.from_pandas(f_test_set)\n",
    "\n",
    "n_train_set = Dataset.from_pandas(n_train_set)\n",
    "\n",
    "\n",
    "BERT_MODEL = 'roberta-base'\n",
    "NUM_LABELS = 3\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, model_max_length=128)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_dataset_train = n_train_set.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_test = f_test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY= 1e-8\n",
    "NUM_TRAIN_EPOCHS = 4\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=NUM_LABELS, \n",
    "                                                            max_length=128)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                    learning_rate=LEARNING_RATE,\n",
    "                                    weight_decay=WEIGHT_DECAY,\n",
    "                                    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                                    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                                    seed = seed_val)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "output = trainer.predict(tokenized_dataset_test)\n",
    "\n",
    "preds = np.argmax(output.predictions, axis=-1)\n",
    "\n",
    "print(classification_report(output.label_ids, preds, digits=4))\n",
    "\n",
    "cf = classification_report(output.label_ids, preds, digits=4, output_dict=True)\n",
    "\n",
    "cm = confusion_matrix(output.label_ids, preds)\n",
    "\n",
    "import json\n",
    "\n",
    "path_to_log = 'predictions'\n",
    "path_to_file = f'{STAR_FILTER} FILTER.json'\n",
    "\n",
    "os.makedirs(path_to_log, exist_ok=True)\n",
    "\n",
    "preds = [int(x) for x in preds]\n",
    "\n",
    "with open(os.path.join(path_to_log, path_to_file), 'w') as f:\n",
    "    json.dump(preds, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfc4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
