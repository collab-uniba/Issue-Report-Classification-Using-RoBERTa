{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ekphrasis\n",
    "!python -m pip install transformers\n",
    "!python -m pip install imblearn\n",
    "!python -m pip install pandas\n",
    "!python -m pip install sklearn\n",
    "!python -m pip install numpy\n",
    "!python -m pip install keras\n",
    "!python -m pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!python -m pip install textattack\n",
    "!python -m pip install transformers\n",
    "!python -m pip install ipywidgets==7.4.2\n",
    "!python -m pip install mlflow\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4f9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import re\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from _datetime import datetime as dt\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import json\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b006440",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "img_1 = re.compile('!\\[(.*)\\]\\(.*\\)')\n",
    "link_1 = re.compile('\\[(.*)\\]\\(.*\\)')\n",
    "link_2 = re.compile('\\[(.*)\\]: [^\\s]+')\n",
    "code_1 = re.compile('(:?`[^`]+`|```[^`]*```)')\n",
    "\n",
    "def preprocess(row):\n",
    "  doc = \"\"\n",
    "  doc += str(row.issue_title)\n",
    "  doc += \" \"\n",
    "  doc += str(row.issue_body)\n",
    "  \n",
    "  return clean_text(doc)\n",
    "\n",
    "def clean_text(text):\n",
    "  cleaned = re.sub(img_1, r'\\1 <img>', text)\n",
    "  cleaned = re.sub(link_1, r'\\1 <url>', cleaned)\n",
    "  cleaned = re.sub(link_2, r'\\1 <url>', cleaned)\n",
    "  cleaned = re.sub(code_1, '<code>', cleaned)\n",
    "  ekph_cleaned = \" \".join(text_processor.pre_process_doc(cleaned))\n",
    "  return ekph_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393056b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rows(df, label_encoder):\n",
    "    df = df.fillna({\n",
    "                        'issue_title': '',\n",
    "                        'issue_body':''                  \n",
    "                   })\n",
    "    df['text'] = df['issue_title'] + df['issue_body']\n",
    "    df['label'] = label_encoder.transform(df['issue_label'])\n",
    "    df = df.filter(['text', 'label'])\n",
    "    df['text'] = [clean_text(text) for text in tqdm(df['text'])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466fdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_to_dataframe(str_representation_of_report):\n",
    "    split_string = [x.split(' ') for x in str_representation_of_report.split('\\n')]\n",
    "    column_names = ['']+[x for x in split_string[0] if x!='']\n",
    "    values = []\n",
    "    for table_row in split_string[1:-1]:\n",
    "        table_row = [value for value in table_row if value!='']\n",
    "        if table_row!=[]:\n",
    "            values.append(table_row)\n",
    "    for i in values:\n",
    "        for j in range(len(i)):\n",
    "            if i[1] == 'avg':\n",
    "                i[0:2] = [' '.join(i[0:2])]\n",
    "            if len(i) == 3:\n",
    "                i.insert(1,np.nan)\n",
    "                i.insert(2, np.nan)\n",
    "            else:\n",
    "                pass\n",
    "    report_to_df = pd.DataFrame(data=values, columns=column_names)\n",
    "    return report_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d9d3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the training set if it does not exist\n",
    "if not os.path.isfile(\"github-labels-top3-803k-train.csv\"):\n",
    "  !curl \"https://tickettagger.blob.core.windows.net/datasets/github-labels-top3-803k-train.tar.gz\" | tar -xz\n",
    "\n",
    "trainset = pd.read_csv(\"github-labels-top3-803k-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4af67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"github-labels-top3-803k-test.csv\"):\n",
    "  !curl \"https://tickettagger.blob.core.windows.net/datasets/github-labels-top3-803k-test.tar.gz\" | tar -xz\n",
    "\n",
    "testset = pd.read_csv(\"github-labels-top3-803k-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06bf8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenc = LabelEncoder()\n",
    "lenc.fit(trainset[\"issue_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8eb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.drop_duplicates(subset=['issue_url'], inplace=True)\n",
    "testset.drop_duplicates(subset=['issue_url'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7d0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..\\dict.json') as f:\n",
    "    memo_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903da128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGE_FILTER = 365 * 4\n",
    "AGE_FILTER = (365 * 4) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = []\n",
    "age = []\n",
    "\n",
    "temp = trainset.copy()\n",
    "for ix, x in tqdm(temp.iterrows()):\n",
    "    stars.append(memo_dict[x['repository_url']][0])\n",
    "    age.append(memo_dict[x['repository_url']][1])\n",
    "    \n",
    "temp['stars'] = stars\n",
    "temp['age'] = [int(x) for x in age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0ef4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df_train = temp.loc[(temp['age']>= AGE_FILTER/4) & (temp['age']<= AGE_FILTER)]\n",
    "filtered_df_train = temp.loc[(temp['age']>= AGE_FILTER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a3ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_train['age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c138a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "starss = []\n",
    "agee = []\n",
    "\n",
    "tempp = testset.copy()\n",
    "for ix, x in tqdm(tempp.iterrows()):\n",
    "    starss.append(memo_dict[x['repository_url']][0])\n",
    "    agee.append(memo_dict[x['repository_url']][1])\n",
    "\n",
    "tempp['stars'] = starss\n",
    "tempp['age'] = agee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73f60ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df_test = tempp.loc[(tempp['age']>= AGE_FILTER/4) & (tempp['age']<= AGE_FILTER)]\n",
    "filtered_df_test = tempp.loc[(tempp['age']>= AGE_FILTER)]\n",
    "#filtered_df_test = tempp.loc[(temp['age']>= AGE_FILTER) & (temp['age']<= AGE_FILTER*4 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2f7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9935c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "print('Using seed: {}'.format(seed_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(trainset['issue_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93559baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_set = preprocess_rows(filtered_df_train, label_encoder)\n",
    "test_set = preprocess_rows(filtered_df_test, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ada64e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = filtered_df_train.groupby(\"issue_label\").size()\n",
    "train_size = [train_size['bug'], train_size['enhancement'], train_size['question']]\n",
    "index = train_size.index(min(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7062bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "under_train_dict = {0:train_size[index] ,1:train_size[index],2:train_size[index]}\n",
    "\n",
    "train_set, _ = RandomUnderSampler(sampling_strategy=under_train_dict, random_state=42).fit_resample(train_set, list(train_set[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af1d2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset.from_pandas(train_set)\n",
    "test_set = Dataset.from_pandas(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d60c688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = 'roberta-base'\n",
    "NUM_LABELS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b68c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, model_max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bb47a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_test = test_set.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbaee021",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY= 1e-8\n",
    "NUM_TRAIN_EPOCHS = 4\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=NUM_LABELS, \n",
    "                                                           max_length=128)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                 learning_rate=LEARNING_RATE,\n",
    "                                 weight_decay=WEIGHT_DECAY,\n",
    "                                 num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                                 per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                                 seed = seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a347de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71928760",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(tokenized_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "579e09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(output.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdeea23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(output.label_ids, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b54c0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = classification_report(output.label_ids, preds, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa16d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "536e1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_train = filtered_df_train.groupby(\"issue_label\").size()\n",
    "size_test = filtered_df_test.groupby(\"issue_label\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(output.label_ids, preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "263cdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path_to_log = 'predictions'\n",
    "path_to_file = f'{AGE_FILTER} FILTER.json'\n",
    "\n",
    "os.makedirs(path_to_log, exist_ok=True)\n",
    "\n",
    "preds = [int(x) for x in preds]\n",
    "\n",
    "with open(os.path.join(path_to_log, path_to_file), 'w') as f:\n",
    "    json.dump(preds, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
